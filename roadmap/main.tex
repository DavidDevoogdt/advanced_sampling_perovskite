\documentclass{article}
\usepackage[utf8]{inputenc}
%\usepackage[dutch]{babel} % English hyphenation etc.

\usepackage[margin=1.5in]{geometry}
\usepackage{url} % Fancy url's
\usepackage{graphicx}
\usepackage{caption}% Captions onder figuur gecentreerdhyp
\usepackage[toc,page]{appendix}
\usepackage{subcaption} % Subfigure environment 
\usepackage{float}
\usepackage{hyperref}
%\usepackage{hyperref}
\usepackage{siunitx} % Elegant eenheden zetten
\usepackage[version=3]{mhchem} % ingeven van chemische fomules
\usepackage{cleveref} % Paragraaf tekens
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{grffile}%double dot in fugure name
\usepackage{textcomp} %gets rid of warnings of gensymb
\usepackage{gensymb}
\usepackage{adjustbox}
\usepackage{booktabs}

\usepackage{amsmath} % Mathematical symbols
\usepackage{amssymb} % Symbols
\usepackage{amsfonts}
\usepackage{listings}

\usepackage{cite}

\title{Roadmap }
\author{David Devoogdt }
\date{}
\begin{document}

\maketitle

\section{Goal}
This roadmap outlines some recent advancements in sampling methods which use as little knowledge as possible to sample a transition between 2 phases. This is necessary because it's not always possible to find a good reaction coordinates. Once a path is known, other methods can be used to find free energy profiles etc.

In \cref{s:gm} (subjectively) interesting groups of methods are listed. In \cref{s:mp} the methods that will be implemented in this repository are selected and discussed. As a model, a toy perovskite system with a 5 atoms unit cell is used. This has 2 stable phases.

\section{Groups of methods}\label{s:gm}

\subsection{Optimal selection of predefined reaction coordinates}

There exist quite some methods to make an optimal selection of a number of predefined collective variables. While this is very useful, it still requires some insight in the reaction mechanism before sampling begins. Therefore, these methods are not very useful without prior knowledge. These methods iteratively select better RC and generate new MD data with the selected RC.

\paragraph{Linear combinations} The most limited form includes making a linear combination of a pool of preselected order parameters.

time-lagged independend component analysis (tICA) \cite{Tiwary2016}. Selects linear combination based on dominant eigenvalue of transition matrix. The functions in tICA themselves can be non-linear (kernel tICA) and faster version exist (e.g. hTICA)

SGOOP \cite{Tiwary2016} optimises based on a different principle (mninmising spactral gap). SGOOP-d improves upon the idea by using commute distance from \cite{noe2016}.

The problem is that this still requires one valid reaction coordinate in the pool for the whole reaction. This is still too difficult for complex reactions

\paragraph{Reinforcement learning based adaptive sampling (REAP) } \cite{Shamsi2018} clusters explored structures. The reaction coordinate is changed and initial structures are selected to explore the most unsampled regions. This can use different coordinates dependend on the place in the reaction.

\subsection{Manifold learning}

Manifold learning assumes that the given high dimensional data is in essence sampled from a low dimensional space. The aim is to transform the data back to a low dimensional manifold without distorting the distances between points. Dimension reduction algorithms tend to fall into two categories; those that seek to preserve the pairwise distance structure amongst all the data samples and those that favor the preservation of local distances over global distance. \cite{McInnes2018}

Successful examples of these techniques include IsoMap \cite{Tenenbaum1995} and DMAPS \cite{Coifman2006} Current state of the art techniques are  t-SNE \cite{vanDerMaarten2008} and  UMAP  \cite{McInnes2018}. The methods from manifold learning are mostly quite mathematical in nature, but broadly applicable.

These methods are generally not directly linked to QM simulations. In essence the points of an MD trajectory form a low dimensional manifold. The idea here is to select alternatingly better collective variables (learnt by the techniques above) and produce new MD trajectories based on these improved CV's

\subsubsection{DMAPS}

Difusion maps (DMAPS)\cite{Coifman2006} construct an affinity matrix K (which measures in some sense how similar 2 configurations are, e.g. with radial basis function kernel). The d largest eigenvectors of normalised K servers as low dimensional coordinates.

Some methods build on this by running short MD simulations, performing a manifold learning technique and spawn new MD simulation outside currently sampled portion of phase space. (see e.g. diffusion-map directed MD (\cite{Preto2014}) and intrinsic map dynamics iMapd\cite{Chiavazzo2017}). This doesn't need any prior knowledge of the system.

Diffusion nets (DNETS) \cite{Mishne2015} uses and encoder and decoder to learn the DMAPS mapping between rounds. This makes the problem better behaved and allows to use the autoencoder as CV. More about autoencoders can be found in next section.

\subsubsection{t-SNE}
t-Distributed Stochastic Neighbor Embedding (t-SNE) \cite{vanDerMaarten2008} centers gaussians around every original point $x_i$ and t-distributions around every point in embedded space $y_i$. The KL differgence is minimised such that $p(i|j) = q(i|j)$ with p the neighbour probability in x space and q in y space. Embedding parameters are learned. \cite{Rydzewski2021} uses WT-MetaD to explore phase space, and uses a NN  map x to y.

Active enhanced sampling \cite{Zhang2018} is similar in spirit: it also minimises KL div between 2 representations.

\subsubsection{UMAP}\label{ss:umap}

The theory behind uniform manifold approximation and projection (UMAP) \cite{McInnes2018} is based on manifold theory and topological data analysis. The paper explains it in language of category theory, and hence this it is certainly too much effort to try to understand it. The computational version is still quite difficult. Luckily there is a reference implementation on github.

In \cite{Trozzi2021}, this method is gauged against PCA, tICA and t-SNE for a biomolecule. The generated CV's are competitive with t-SNE but have smaller time complexity.

\subsection{Information bottleneck }

Many algorithms use an autoencoder as dimensionality reduction. Mostly, the sampled data is put trough a neural network with a bottleneck (layer with few (latent) variables).

\subsubsection{VAMPnets  }
In VAMPnets \cite{Mardt2017}, info and time lagged info is put through 2 network lobes which learn latent variables $\chi_0$ $\chi_1$ related by matrix K (function of $\chi_x$). This constructs a MSM.

\subsubsection{autoencoders}

The neural network is trained to recover its input as autput (autoencoder). The learned latent variables are used in a new round of enhanced sampling. This continues until the CV's do not change any more \cite{Chen2018}.

As a variation, \cite{Hooft2021} trains 2 genetic algorithms: 1 selects CV's from pool of possible CV's (encoder), and a second NN acts as decoder

The autoencoders are much less prone to noise if gaussian noise is added to the latent variables (Variational auto encoders) \cite{Shoberl2019,Hernandez2017,Bozkurt2020}.
Predicting time lagged output also improves reliability, as only slow modes are learned \cite{Wehmeyer2018,Hernandez2017,Shoberl2019}

The latest methods use bayesian learning to construct the network. These methods are all able to construct good course grained markov state models. State Predictive Information Bottleneck (SPIB) \cite{Wang2021} learns state label instead of configuration and constructs MSM's. It able to predict committor probability. SPIB uses mixture of gaussian as prior. \cite{Shoberl2019} uses autoencoders with good results. Gaussian mixture VAE \cite{Bozkurt2020}: Uses gaussians mixture (sum of gaussians with different center) as prior distribution.

\subsection{Normalising Flows}\label{s:BG}

Another recent parameter free technique is Boltzmann generators \cite{Noe2019}, rooted in the normalising flows (ML technique). The idea is to make an invertible map between a prior distribution (e.g. gaussian) and the desired Boltzmann distribution $p(x)=e^{-\beta U(x)}$. This can be thought of as scaling the distribution around every point with a suitable jacobian. The mappping function is an invertible neural network. The training of these network happen in 2 different ways:

1) sample vectors z from prior distribution and transforms them to real configurations $X=F_{zx}(z)$. Each round, the KL divergence between the sampled distribution $p_{X}(x)$ and the real distribution $p(x)=e^{-\beta U(x)}$ is minimised by adjusting the weights and biases of the NN map. A round requires around 1000 samples from prior.

2) Valid samples are taken form an existing MD trajectory, and are transformed to the prior space $Z = F_{xz}(x)$. Here, the Maximal likelyhood is minimised (compared to the prior gaussian distribution)

Some improvements were already made. In \cite{kohler2019}, some relevant physical symmetries of the physical space X are taken into account (i.e. permutations of atoms, global rotations,...). In \cite{Dibak2021}, the flow is temperature steerable, meaning that the output distribution $F_{xz}$ can be scaled based on the desired temperature. This is possible because $p(x,t') = [e^{-\frac{U(x)}{k t} } ]^{\frac{t}{t'}} =  p(x,t)^{t/t'} $. By suitably selecting the map, the prior can be sampled and trained at any temp. Metropolis algorithm is needed to select unbiased samples

\subsection{ General Adversarial Networks}
Another technique from ML community is called adversarial learning. This is applied in the following paper Targeted Adversarial Learning Optimized Sampling (TALOS) \cite{Zhang2019} to advanced sampling methods. This is one of the only papers that uses GAN architecture. It's still too early at this point to do somehting with it

\subsection{Path-based methods}

\paragraph{Transition Path Sampling (TPS)}

Transition path sampling takes one known path between 2 phases and from there samples a number of reaction paths between the states to determine the statistical properties. Path are accepted based on a metropolis sampling criterium. See e.g. \cite{Dellago1998}.

In the recent paper \cite{Jung2019}, the committor probability is stored in an NN.

There are some difficulties related to TPS: only one path can be sampled, slow, and initial path needs to be known (and be a faithful path)

\paragraph{Metadynamics of paths} \cite{Mandelli2020} combines metaD and path sampling. Seems slow for large systems?

\section{Selected methods for Perovskites}\label{s:mp}

This section lists the methods that will be tested for perovskite toy model. The idea is to choose a state-of-the-art model from each different type and compare their performance.

\subsection{Boltzmann generators}

Bolzmann generators are explained in \cref{s:BG}, an excelent implementation can be found here \href{https://github.com/noegroup/bgflow}{git}.

The group of Frank Noé is the only group which develops Boltzmann generators/normalising flows at the moment, and this git repository contains all the variants (equivariant, temperature steering).

While all the components to make a normalising flow where in place, some modifications were necessary to adapt it for perovskites. Some of them are listed below:

\paragraph{Implementation work} Some work was needed to couple the generator to CP2K (through ASE). This works now stable and reliable. The training of the Boltzmann generators works as intended. The git repo also stores everything cleanly and save all the used parameters to make everything reproducible. It's connected to weigts and biases for easy comparison of experiments

\paragraph{Flow design} The flows original papers uses realNVP flows, which splits the parameter in 2 groups and updates one half with an affine transformation (neural network) conditional on the other half. This is very arbitrarily. The current network makes a block for each atom/cell (feature) to all other features individually. Identical combinations of atoms get the same connection. This is very similar in spirit to message passing networks, such as Schnet. There is also a layer which scales and shifts the inputs to make it more similar to a gaussian prior, by choosing the variance different for e.g. the angles than the cell sizes etc.

\paragraph{Sampling} At the moment the initial data is generated with a markov chain monte carlo process. (unrelated: it took some time to read about monte carlo algorithms, optimal sampling parameters, ...). This is not necessary and MD methods are also perfectly fine.

\paragraph{phase transition} In the original paper, random points in the prior distribution were connected to find the path in real space. For a  crystal, it is less clear which point to connect, as in theory any of the I atoms in phase 1 could go to any of the other atoms in phase 2. At the moment, a simple solution seems to find the shortest path in prior space between all equivalent points (i.e. permutations of I atoms). For bigger unit cells, some work will be needed to keep this manageable

\paragraph{Feeling for parameters} As this kind of work is quite new to me (distribution sampling, mcmc/MD) and the example from the papers are very different, it's still quite difficult to develop a feeling for good parameters. Also training neural networks requires some intuition which is sometimes lacking

\paragraph{Results} It's still to early too talk about results. In hindsight, my learned models were fitted on bogus initial data (sampler generated to many invalid configurations which were all rejected by the mcmc)

\paragraph{Improvements} As noted earlier, the current architecture somewhat resembles schnet. There is definitely more things that can be borrowed from MLP's.

\subsection{REAP} \cite{Shamsi2018},\href{https://github.com/ShuklaGroup/REAP-ReinforcementLearningBasedAdaptiveSampling}{git}.

This is one of the few methods left in the list that explicitly uses preselected coordinates. This might be handy if the phase diagram needs to be compared against other methods.

\paragraph{Implementation} TODO

\subsection{UMAP} \cref{ss:umap} \href{https://github.com/lmcinnes/umap}{git}

Umap seems the most promising and scalable method in manifold mapping.

\paragraph{Implementation} TODO

\subsection{SPIB} \cite{Wang2021} \href{https://github.com/tiwarylab/State-Predictive-Information-Bottleneck}{git}

This uses all recent advances in for autoencoders: bayesians interference learning, time lagging and it produces a reliable MSM

\paragraph{Implementation} TODO

\subsection{Path based methods} skip for the moment, because accurate initial trajectory is needed.

\bibliographystyle{ieeetr}
\bibliography{bib}
\end{document}
